<!DOCTYPE HTML>
<!--
	Story by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Dhruv Verma</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta property="og:image" content="images/preview.png" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" href="images/favicon.png">
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-181810599-1"></script>
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'UA-181810599-1');
	</script>

	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="divided">

				<!-- One -->
					<section class="banner style1 orient-right content-align-left image-position-left fullscreen onload-image-fade-in onload-content-fade-right">
						<div class="content" style>
							<h1>
								<b>Namasté</b><br>
							I'm Dhruv Verma</h1>
							<p>	I'm a Ph.D. student at the University of Toronto, advised by Prof. 
								<a href="https://mariakakis.github.io/" target="_blank">Alex&nbsp;Mariakakis</a>. 
								Broadly, my research involves advancing the capabilities of computing devices to 
								sense and perceive information (inertial, visual, acoustic, physiological,
								and others) relevant to understanding the human body, behaviour and the surrounding context. 
								<br><br>
								During my doctoral journey, I've developed a keen interest in 
								<a href="https://en.wikipedia.org/wiki/Computational_imaging" target="_blank">computational&nbsp;imaging</a>. 
								Specifically, in building computing infrastructure that leverages ubiquitous devices to 
								provide a visual perspective of the world that extends beyond human perception. In realizing 
								these systems, I bring together my expertise in signal and image processing, engineering, 
								and machine learning with a human-centred approach at the core.
								<br><br>
								My endeavours are driven by a desire to unlock new possibilities and improve existing tasks enabled by 
								sensing systems, particularly those focused on improving health outcomes and enhancing human-computer interaction. 
								</p>

							<!-- <p class="major">I’m a Ph.D. student at the University of Toronto advised by Prof. 
								<a href="https://mariakakis.github.io/" target="_blank">Alex Mariakakis</a>, 
								working at the intersection of emerging ubiquitous technologies and human-centered computing. 
								My research focuses on building practical, unobtrusive, and deployable sensing systems that 
								have applications in healthcare, context awareness, virtual & augmented reality, and novel interactions.</p>
							<p class="major">Prior to my studies at U of T, I graduated with a Bachelors in Computer Science from 
								<a href="https://iiitd.ac.in/" target="_blank">IIIT Delhi</a>. 
								As an undergrad, I've had the pleasure to work with
								Prof. <a href="https://amanparnami.com" target="_blank">Aman Parnami</a>, 
								Prof. <a href="https://jainendra.in" target="_blank">Jainendra Shukla</a>, and
								Prof. <a href="http://www.mayankgoel.com" target="_blank">Mayank Goel</a>.</p>	 -->
							<p><b>Update: I'm seeking research internships for 2024. If you're recruiting and think 
								I'd be a good fit, please don't hesitate to reach out!
								</b></p>

							<ul class="icons">
								<li><a href="mailto:dhruvverma@cs.toronto.edu" target="_blank" class="icon style2 fa-envelope"><span class="label">Email</span></a></li>
								<li><a href="https://twitter.com/dhruvverma_" target="_blank" class="icon brands style2 fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="https://www.linkedin.com/in/~dhruvverma/" target="_blank" class="icon brands style2 fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
								<li><a href="https://scholar.google.com/citations?user=KJgi6GMAAAAJ" target="_blank" class="icon brands style2 fa-google"><span class="label">Scholar</span></a></li>
							</ul>

							<ul class="actions fit small">
								<li><a href="docs/CV.pdf" target="_blank" class="button fit">Curriculum Vitae</a></li>
								<li><a href="#research" class="button smooth-scroll">Research</a></li>
							</ul>
						</div>
						<div class="image">
							<img src="images/potrait.jpg" alt="potrait" style="object-position: right;"/>
						</div>
					</section>


				<!-- Filler -->
					<section class="wrapper style1" id="research">
						<section class="wrapper style1 align-center">
							<div class="inner">
								<h2>Selected Research</h2>
								<p class="major">I publish my work at premier venues in interactive systems, sensing, and human-computer interaction. 
									<br> Below are the highlights of my work:
								</p>
							</div>
						</section>

					</section>
				
				<!-- New Project (Right)  -->
				<section class="spotlight style1 orient-right content-align-left image-position-center onscroll-image-fade-in" id="new project"
				style="height: 30rem;">
					<div class="content">
						<header>
							<h2><b>Hyperspectral Imaging, At Your Fingertips!</b></h2>
							<p><b>Dhruv Verma</b> et al. <u>Under Submission</u></p>
						</header>
						<p> More details coming soon.
							</p>
						<ul class="actions">
							<!-- <li><a href="docs/ExpressEar_ACM_UbiComp_2021.pdf" target="_blank" class="button">Paper</a></li>
							<li><a href="https://www.youtube.com/watch?v=AbSuQHvUABc" target="_blank" class="button">Presentation</a></li> -->
						</ul>
					</div>
					<div class="image" style="background-color: inherit;">
						<img src="images/spotlight_teaser.jpg" alt="Teaser" style="height: 100%; width: 100%; object-fit: fit-down; object-position: left;"/>
					</div>
				</section>


				<!-- AttentioNet (Right)  -->
				<section class="spotlight style1 orient-left content-align-left image-position-center onscroll-image-fade-in" id="attentionet">
					<div class="content">
						<header>
							<h2><b>AttentioNet:</b> Classifying Student Attention Types With EEG</h2>
							<p><b>Dhruv Verma</b>, Sejal Bhalla, Sai Santosh, Saumya Yadav, Aman Parnami, and Jainendra Shukla. <u>IEEE&nbsp;ACII&nbsp;2023</u></p>
						</header>
						<p> Human attention is often characterized by using simplistic computational models, such as a binary: attentive or non-attentive, or 
							degree: low, medium, high levels. However, the Clinical Model of Attention (Sohlberg and Mateer, 2001) suggests a more nuanced
							perspective. According to this model, there are five distinct attention states: focused, selective, sustained, alternating, and divided attention.
							In this research, we show that different attention types exhibit unique neural activity patterns measurable with EEG. 
							Deep learning methods may be used to create comprehensive models of human attention considering these distinctions.
							</p>
						<ul class="actions">
							<li><a href="#coming_soon" target="_blank" class="button">Paper</a></li>
							<li><a href="docs/AttentioNet_ACII_2023.pdf" target="_blank" class="button">PDF</a></li>
						</ul>
					</div>
					<div class="image" style="background-color: inherit;">
						<img src="images/spotlight_attentionet.png" alt="AttentioNet Illustration" style="height: 100%; width: 100%; object-fit: fit-down; object-position: right;"/>
					</div>
				</section>


				<!-- SAMoSA (Left)  -->
				<section class="spotlight style1 orient-right content-align-left image-position-center onscroll-image-fade-in" id="samosa">
					<div class="content">
						<header>
							<h2><b>SAMoSA:</b> Sensing Activities With Motion and Subsampled Audio</h2>
							<p>Vimal Mollyn, Karan Ahuja, <b>Dhruv Verma</b>, Chris Harrison, and Mayank Goel. <u>ACM&nbsp;UbiComp/IMWUT&nbsp;2022</u></p>
						</header>
						<p> SAMoSA is a human-activity recognition system that leverages inertial sensors and low-fidelity 
							audio signals from consumer-grade smartwatches. By using low-fidelity audio (sampled at less than 1 kHz) instead of typical
							audio (greater than 16 kHz), it offers a solution that is power-efficient and privacy-preserving, since speech becomes 
							unintelligible at these low sampling rates. Despite using lower fidelity signals, SAMoSA manages to achieve a performance 
							comparable to systems using 16 kHz audio data when it comes to classifying various daily activities, across different contexts.   
							</p>
						<ul class="actions">
							<li><a href="https://doi.org/10.1145/3550284" target="_blank" class="button">Paper</a></li>
							<li><a href="docs/SAMoSA_ACM_Ubicomp_2022.pdf" target="_blank" class="button">PDF</a></li>
							<li><a href="https://youtu.be/QEJBaD71x_s" target="_blank" class="button">Video</a></li>
							<li><a href="https://github.com/cmusmashlab/SAMoSA" target="_blank" class="button">Code</a></li>
							

						</ul>
					</div>
					<div class="image" style="background-color: inherit;">
						<img src="images/spotlight_samosa.png" alt="SAMoSA Illustration" style="height: 100%; width: 100%; object-fit: fit-down; object-position: left;"/>
					</div>
				</section>

				<!-- ExpressEar (Right)  -->
					<section class="spotlight style1 orient-left content-align-left image-position-center onscroll-image-fade-in" id="expressear">
						<div class="content">
							<header>
								<h2><b>ExpressEar:</b> Sensing Fine-Grained Facial Expressions with Earables</h2>
								<p><b>Dhruv Verma</b>, Sejal Bhalla, Dhruv Sahnan, Jainendra Shukla, and Aman Parnami. <u>ACM&nbsp;UbiComp/IMWUT&nbsp;2021</u></p>
							</header>
							<p> ExpressEar is a system that takes advantage of inertial sensors found on everyday wireless earbuds to detect subtle
								facial expressions. It is based on the 
								<a href="https://www.paulekman.com/facial-action-coding-system/">Facial Action Coding System (FACS)</a>, 
								and has the capability to detect minute facial movements, also known as facial action units, with high accuracy. ExpressEar offers a high fidelity, privacy-preserving, and an 
								unobtrusive alternative to camera-based approaches, suitable for both stationary and mobile contexts. 
								Its applications span various fields, including affective computing, face gesture recognition, 
								animation and graphics, among others.  
								</p>
							<ul class="actions">
								<li><a href="https://doi.org/10.1145/3478085" target="_blank" class="button">Paper</a></li>
								<li><a href="docs/ExpressEar_ACM_UbiComp_2021.pdf" target="_blank" class="button">PDF</a></li>
								<li><a href="https://www.youtube.com/watch?v=AbSuQHvUABc" target="_blank" class="button">Presentation</a></li>
							</ul>
						</div>
						<div class="image" style="background-color: inherit;">
							<img src="images/spotlight_expressear.jpg" alt="ExpressEar Illustration" style="height: 100%; width: 100%; object-fit: fit-down; object-position: right;"/>
						</div>
					</section>

				<!-- Fashionist (Left) -->
				<section class="spotlight style1 orient-right content-align-left image-position-center onscroll-image-fade-in" id="first">
					<div class="content">
						<header>
							<h2><b>Fashionist:</b> Personalising Outfit Recommendation for Cold-Start Scenarios</h2>
							<p><b>Dhruv Verma</b>, Kshitij Gulati, and Rajiv Ratn Shah. <u>ACM&nbsp;Multimedia&nbsp;2020</u>, <u>IEEE&nbsp;BigMM&nbsp;2020</u></p>
						</header>
						<p> Fashionist is an data-driven fashion recommendation system that utilizes a small set of style examples to model user
							preferences. It harvests knowledge related to fashion concepts, semantics, and context through a joint training process, 
							which includes predicting fashion category (e.g., jacket, pant, skirt, etc.) and attributes (e.g., 
							sleeve length, color, texture, etc.) on publicly available fashion datasets.      
							</p>
						<ul class="actions">
							<li><a href="https://doi.org/10.1109/BigMM50055.2020.00043" target="_blank" class="button">Paper</a></li>
							<li><a href="docs/Addressing_IEEE_BigMM_2020.pdf" target="_blank" class="button">PDF</a></li>
							<li><a href="https://www.youtube.com/watch?v=kuKgPCkoPy0" target="_blank" class="button">Video</a></li>
							<li><a href="https://doi.org/10.1145/3394171.3414446" target="_blank" class="button">Demo</a></li>
						</ul>
					</div>
					<div class="image" style="background-color: inherit;">
						<img src="images/spotlight_fashionist.png" alt="Fashionist Illustration" style="height: 100%; width: 100%; object-fit: fit; object-position: left;"/>
					</div>
				</section>

				
				<!-- Get in touch -->
					<!-- <section class="wrapper style1 align-center">
						<div class="inner medium">
							<h2>Get in touch</h2>
							<form method="post" action="#">
								<div class="fields">
									<div class="field half">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" value="" />
									</div>
									<div class="field half">
										<label for="email">Email</label>
										<input type="email" name="email" id="email" value="" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="6"></textarea>
									</div>
								</div>
								<ul class="actions special">
									<li><input type="submit" name="submit" id="submit" value="Send Message" /></li>
								</ul>
							</form>

						</div>
					</section> -->

				<!-- Footer -->
					<footer class="wrapper style1 align-center ">

						
						<div class="inner">

							<h2>Contact Information</h2>
								<p>
									For any inquiries contact: <b>dhruvverma&nbsp;at&nbsp;cs&nbsp;dot&nbsp;toronto&nbsp;dot&nbsp;edu</b>.<br> 
									I'm a member of the <a href="https://chai.cs.toronto.edu/" target="_blank">Computational Health and Interaction Lab</a> and 
									<a href="https://www.dgp.toronto.edu/" target="_blank">Dynamic Graphics Project Lab</a> at U of T. <br> 
									Questions regarding my lab can be directed to <b>mariakakis&nbsp;at&nbsp;cs&nbsp;dot&nbsp;toronto&nbsp;dot&nbsp;edu</b>.
								</p>

								<center>
									<!-- <div class="image" style="background-color: inherit;"> -->
									<img src="images/logo_chai_lab.png" alt="CHAI lab logo" style="height: 30%; width: 30%; vertical-align:middle; object-position: left;"/>
									<img src="images/logo_dgp_lab.png" alt="DGP lab logo" style="height: 30%; width: 30%; object-position: center; vertical-align:middle; margin-right: 20px;"/>
									<img src="images/logo_uoft.png" alt="U of T logo" style="height: 22%; width: 22%; vertical-align:middle; object-position: right;"/>
								<!-- </div> -->
							</center>
		
								
							<!-- <ul class="icons">
								<li><a href="#" class="icon brands style2 fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands style2 fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands style2 fa-instagram"><span class="label">Instagram</span></a></li>
								<li><a href="#" class="icon brands style2 fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
								<li><a href="#" class="icon style2 fa-envelope"><span class="label">Email</span></a></li>
							</ul> -->
							<p>&copy; 2023 Dhruv Verma. Design Inspiration: <a href="https://html5up.net">HTML5 UP</a>.</p>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>